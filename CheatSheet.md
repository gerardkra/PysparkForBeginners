### üîπ Qu‚Äôest-ce qu‚Äôun RDD ?

**RDD** (Resilient Distributed Dataset) est la structure de base de Spark.
C‚Äôest un **ensemble distribu√© d‚Äô√©l√©ments** r√©partis sur plusieurs n≈ìuds du cluster, permettant un **traitement parall√®le**.

Caract√©ristiques :

* **Immuable** ‚Üí une fois cr√©√©, on ne peut pas le modifier.
* **Tol√©rant aux pannes** ‚Üí il se r√©g√©n√®re automatiquement en cas d‚Äôerreur.
* **Distribu√©** ‚Üí les calculs se font en parall√®le sur plusieurs machines.

---

### üîπ Types d‚Äôop√©rations sur les RDD

Il existe **deux grandes familles** d‚Äôop√©rations :

1. **Transformations** ‚Üí cr√©ent un **nouvel RDD** √† partir d‚Äôun existant.
   ‚öôÔ∏è *Exemples :* `map()`, `filter()`, `groupBy()`, `join()`

2. **Actions** ‚Üí d√©clenchent l‚Äôex√©cution des transformations et **renvoient un r√©sultat** au driver.
   üì§ *Exemples :* `count()`, `collect()`, `reduce()`, `foreach()`

> üí° Les transformations sont **paresseuses** (lazy) : elles ne s‚Äôex√©cutent que lorsqu‚Äôune action est appel√©e.

---

## ‚ö° PySpark RDD Cheatsheet

### üèóÔ∏è Cr√©ation d‚Äôun RDD

```python
from pyspark import SparkContext
sc = SparkContext("local", "app")
words = sc.parallelize([
   "scala", "java", "hadoop", "spark", "akka",
   "spark vs hadoop", "pyspark", "pyspark and spark"
])
```
``sc.parallelize()``
cr√©e un RDD √† partir d'une collection Python locale (comme une liste).

---

### üîÑ TRANSFORMATIONS (cr√©ent un nouveau RDD)

| Op√©ration        | Description                                     | Exemple                               | R√©sultat                                |
| ---------------- | ----------------------------------------------- | ------------------------------------- | --------------------------------------- |
| `map(f)`         | Applique une fonction √† chaque √©l√©ment          | `rdd.map(lambda x: (x, 1))`           | `[("scala",1), ("java",1), ...]`        |
| `filter(f)`      | Garde les √©l√©ments qui respectent une condition | `rdd.filter(lambda x: 'spark' in x)`  | `["spark", "pyspark", ...]`             |
| `groupByKey()`   | Regroupe les valeurs par cl√©                    | `rdd.groupByKey()`                    | `{cl√©: [valeurs]}`                      |
| `reduceByKey(f)` | Combine les valeurs ayant la m√™me cl√©           | `rdd.reduceByKey(lambda a,b: a+b)`    | `[(cl√©, somme)]`                        |
| `join(rdd2)`     | Joint deux RDD par cl√©                          | `x.join(y)`                           | `[("spark", (1,2)), ("hadoop", (4,5))]` |
| `flatMap(f)`     | Comme `map`, mais ‚Äúaplati‚Äù les r√©sultats        | `rdd.flatMap(lambda x: x.split(" "))` | liste d‚Äô√©l√©ments individuels            |

---

### üöÄ ACTIONS (d√©clenchent l‚Äôex√©cution et renvoient un r√©sultat)

| Op√©ration    | Description                                            | Exemple              | R√©sultat                      |
| ------------ | ------------------------------------------------------ | -------------------- | ----------------------------- |
| `count()`    | Nombre d‚Äô√©l√©ments                                      | `rdd.count()`        | `8`                           |
| `collect()`  | Retourne tous les √©l√©ments du RDD                      | `rdd.collect()`      | `["scala", "java", ...]`      |
| `foreach(f)` | Applique une fonction √† chaque √©l√©ment (pas de retour) | `rdd.foreach(print)` | affiche chaque mot            |
| `reduce(f)`  | Combine les √©l√©ments via une fonction binaire          | `nums.reduce(add)`   | `15`                          |
| `first()`    | Premier √©l√©ment du RDD                                 | `rdd.first()`        | `"scala"`                     |
| `take(n)`    | Retourne les `n` premiers √©l√©ments                     | `rdd.take(3)`        | `["scala", "java", "hadoop"]` |

---

### üíæ PERSISTENCE

| Op√©ration   | Description                                                     | Exemple         | R√©sultat         |
| ----------- | --------------------------------------------------------------- | --------------- | ---------------- |
| `cache()`   | Stocke le RDD en m√©moire (niveau par d√©faut)                    | `rdd.cache()`   | RDD mis en cache |
| `persist()` | Stocke avec un niveau choisi (`MEMORY_ONLY`, `DISK_ONLY`, etc.) | `rdd.persist()` | RDD persistant   |
| `is_cached` | V√©rifie si le RDD est en cache                                  | `rdd.is_cached` | `True / False`   |

---

### üß© Commandes Spark

| Action                     | Commande                                 |
| -------------------------- | ---------------------------------------- |
| Ex√©cuter un script PySpark | `$SPARK_HOME/bin/spark-submit script.py` |

---

### üí° Exemple complet

```python
from pyspark import SparkContext
from operator import add

sc = SparkContext("local", "Example App")
words = sc.parallelize(["spark", "hadoop", "pyspark", "java"])

# Transformation
filtered = words.filter(lambda x: 'spark' in x)
mapped = filtered.map(lambda x: (x, 1))

# Action
print("Filtered:", filtered.collect())
print("Mapped:", mapped.collect())
print("Count:", mapped.count())

nums = sc.parallelize([1, 2, 3, 4, 5])
print("Sum:", nums.reduce(add))
```
